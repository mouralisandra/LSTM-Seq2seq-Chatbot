{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "31196d32",
      "metadata": {
        "id": "31196d32"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "from tensorflow.keras import layers , activations , models , preprocessing\n",
        "from tensorflow.keras import preprocessing , utils\n",
        "from gensim.models import Word2Vec\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "d6c5c256",
      "metadata": {
        "id": "d6c5c256"
      },
      "outputs": [],
      "source": [
        "lines = open('data/movie_lines.txt',  encoding='utf-8', errors ='ignore').read().split('\\n')\n",
        "conversations_lines = open('data/movie_conversations.txt',  encoding='utf-8', errors ='ignore').read().split('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a73eb1e6",
      "metadata": {
        "id": "a73eb1e6",
        "outputId": "2614c878-b39e-476b-91e4-500f3181c819",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not!',\n",
              " 'L1044 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ They do to!',\n",
              " 'L985 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I hope so.',\n",
              " 'L984 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ She okay?',\n",
              " \"L925 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Let's go.\"]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "lines[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "33804dc6",
      "metadata": {
        "id": "33804dc6",
        "outputId": "35b59a30-f5ce-4096-8765-a750d68b3d6d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L194', 'L195', 'L196', 'L197']\",\n",
              " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L198', 'L199']\",\n",
              " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L200', 'L201', 'L202', 'L203']\",\n",
              " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L204', 'L205', 'L206']\",\n",
              " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L207', 'L208']\"]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "conversations_lines[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "621473e5",
      "metadata": {
        "id": "621473e5"
      },
      "outputs": [],
      "source": [
        "# create a dictionary with key = id and value = text\n",
        "id2line = {}\n",
        "for line in lines:\n",
        "    _line = line.split(' +++$+++ ')\n",
        "    if len(_line) == 5:\n",
        "        id2line[_line[0]] = _line[4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "fd6fbf58",
      "metadata": {
        "id": "fd6fbf58"
      },
      "outputs": [],
      "source": [
        "# create a list of all conversations' lines\n",
        "# lsit of lists\n",
        "conversations = [ ]\n",
        "for line in conversations_lines:\n",
        "    _line = line.split(' +++$+++ ')[-1][1:-1].replace(\"'\",\"\").replace(\" \",\"\")\n",
        "    conversations.append(_line.split(','))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "863affa3",
      "metadata": {
        "id": "863affa3",
        "outputId": "fbc11450-13ec-45fa-866c-e366bdf86452",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['L194', 'L195', 'L196', 'L197'],\n",
              " ['L198', 'L199'],\n",
              " ['L200', 'L201', 'L202', 'L203'],\n",
              " ['L204', 'L205', 'L206'],\n",
              " ['L207', 'L208']]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "conversations[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "f194d4aa",
      "metadata": {
        "id": "f194d4aa"
      },
      "outputs": [],
      "source": [
        "# sort the lines into inupt texts and output texts\n",
        "inputs = []\n",
        "outputs = []\n",
        "\n",
        "for conversation in conversations:\n",
        "    for i in range(len(conversation)-1):\n",
        "        inputs.append(id2line[conversation[i]])\n",
        "        outputs.append(id2line[conversation[i+1]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "502bd36e",
      "metadata": {
        "id": "502bd36e",
        "outputId": "4cb4d167-9dca-4479-dfbd-0035e6e0f21e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\n",
            "Well, I thought we'd start with pronunciation, if that's okay with you.\n",
            "\n",
            "Well, I thought we'd start with pronunciation, if that's okay with you.\n",
            "Not the hacking and gagging and spitting part.  Please.\n",
            "\n",
            "Not the hacking and gagging and spitting part.  Please.\n",
            "Okay... then how 'bout we try out some French cuisine.  Saturday?  Night?\n",
            "\n",
            "You're asking me out.  That's so cute. What's your name again?\n",
            "Forget it.\n",
            "\n",
            "No, no, it's my fault -- we didn't have a proper introduction ---\n",
            "Cameron.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# load a couple of sorted input-output texts\n",
        "for i in range(5):\n",
        "    print(inputs[i])\n",
        "    print(outputs[i]+'\\n')\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "387419a6",
      "metadata": {
        "id": "387419a6",
        "outputId": "082d134b-3302-46ac-c53e-43a1bae8f2c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "221616\n",
            "221616\n"
          ]
        }
      ],
      "source": [
        "print(len(inputs))\n",
        "print(len(outputs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "e0065099",
      "metadata": {
        "id": "e0065099"
      },
      "outputs": [],
      "source": [
        "# change word format and remove unnecessary characters\n",
        "def clean_text(text):\n",
        "\n",
        "    text = text.lower()\n",
        "    \n",
        "    text = re.sub(r\"i'm\", \"i am\", text)\n",
        "    text = re.sub(r\"he's\", \"he is\", text)\n",
        "    text = re.sub(r\"she's\", \"she is\", text)\n",
        "    text = re.sub(r\"it's\", \"it is\", text)\n",
        "    text = re.sub(r\"that's\", \"that is\", text)\n",
        "    text = re.sub(r\"what's\", \"that is\", text)\n",
        "    text = re.sub(r\"where's\", \"where is\", text)\n",
        "    text = re.sub(r\"how's\", \"how is\", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"\\'d\", \" would\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"won't\", \"will not\", text)\n",
        "    text = re.sub(r\"can't\", \"cannot\", text)\n",
        "    text = re.sub(r\"n't\", \" not\", text)\n",
        "    text = re.sub(r\"n'\", \"ng\", text)\n",
        "    text = re.sub(r\"'bout\", \"about\", text)\n",
        "    text = re.sub(r\"'til\", \"until\", text)\n",
        "    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n",
        "    \n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "5cc0d568",
      "metadata": {
        "id": "5cc0d568"
      },
      "outputs": [],
      "source": [
        "# clean the data\n",
        "clean_inputs = []\n",
        "for text in inputs:\n",
        "    clean_inputs.append(clean_text(text))\n",
        "    \n",
        "clean_outputs = []    \n",
        "for text in outputs:\n",
        "    clean_outputs.append(clean_text(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "6821c0f0",
      "metadata": {
        "id": "6821c0f0",
        "outputId": "5aa004de-0f5a-45eb-c9a0-fdac86ae142e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "can we make this quick  roxanne korrine and andrew barrett are having an incredibly horrendous public break up on the quad  again\n",
            "well i thought we would start with pronunciation if that is okay with you\n",
            "\n",
            "well i thought we would start with pronunciation if that is okay with you\n",
            "not the hacking and gagging and spitting part  please\n",
            "\n",
            "not the hacking and gagging and spitting part  please\n",
            "okay then how about we try out some french cuisine  saturday  night\n",
            "\n",
            "you are asking me out  that is so cute that is your name again\n",
            "forget it\n",
            "\n",
            "no no it is my fault  we did not have a proper introduction \n",
            "cameron\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# load a couple of sorted input-output texts\n",
        "for i in range(5):\n",
        "    print(clean_inputs[i])\n",
        "    print(clean_outputs[i]+'\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "99d3b05a",
      "metadata": {
        "id": "99d3b05a"
      },
      "outputs": [],
      "source": [
        "# remove input and output texts that are shorter than 2 words and longer than 8 words.\n",
        "min_line_length = 2\n",
        "max_line_length = 4\n",
        "\n",
        "# filter out the input texts that are too short/long\n",
        "short_inputs_temp = []\n",
        "short_outputs_temp = []\n",
        "\n",
        "i = 0\n",
        "for text in clean_inputs:\n",
        "    if len(text.split()) >= min_line_length and len(text.split()) <= max_line_length:\n",
        "        short_inputs_temp.append(text)\n",
        "        short_outputs_temp.append(clean_outputs[i])\n",
        "    i += 1\n",
        "\n",
        "# filter out the output texts that are too short/long\n",
        "short_inputs = []\n",
        "short_outputs = []\n",
        "\n",
        "i = 0\n",
        "for text in short_outputs_temp:\n",
        "    if len(text.split()) >= min_line_length and len(text.split()) <= max_line_length:\n",
        "        short_outputs.append(text)\n",
        "        short_inputs.append(short_inputs_temp[i])\n",
        "    i += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "fefcdd91",
      "metadata": {
        "scrolled": true,
        "id": "fefcdd91",
        "outputId": "03a04a71-63db-4f79-b420-fcb20de378db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# of questions: 12682\n",
            "# of answers: 12682\n",
            "% of data used: 6.0%\n"
          ]
        }
      ],
      "source": [
        "# Compare the number of lines we will use with the total number of lines.\n",
        "print('# of questions:', len(short_inputs))\n",
        "print('# of answers:', len(short_outputs))\n",
        "print('% of data used: {}%'.format(round(len(short_inputs)/len(inputs),2)*100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "4ff41f42",
      "metadata": {
        "id": "4ff41f42"
      },
      "outputs": [],
      "source": [
        "# add the end of sentence token to the end of every output.\n",
        "# add the go token to the beginning of every output.\n",
        "for i in range(len(short_outputs)):\n",
        "    short_outputs[i] = '<GO>'+short_outputs[i]+' <EOS>'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "ee157ee9",
      "metadata": {
        "id": "ee157ee9",
        "outputId": "a425f185-564b-4332-d1e4-a085f75b7e49",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VOCAB SIZE : 7786\n"
          ]
        }
      ],
      "source": [
        "tokenizer = preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts(short_inputs + short_outputs)\n",
        "VOCAB_SIZE = len( tokenizer.word_index )+1\n",
        "print( 'VOCAB SIZE : {}'.format( VOCAB_SIZE ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "4111c8c1",
      "metadata": {
        "id": "4111c8c1"
      },
      "outputs": [],
      "source": [
        "vocab = []\n",
        "for word in tokenizer.word_index:\n",
        "    vocab.append(word)\n",
        "\n",
        "def tokenize(sentences):\n",
        "    tokens_list = []\n",
        "    vocabulary = []\n",
        "    for sentence in sentences:\n",
        "        tokens = sentence.split()\n",
        "        vocabulary += tokens\n",
        "        tokens_list.append(tokens)\n",
        "    return tokens_list, vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "9e548bac",
      "metadata": {
        "id": "9e548bac",
        "outputId": "53a9d7ad-c5cb-44cf-81fb-9dccc508f0b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(12682, 6) 6\n"
          ]
        }
      ],
      "source": [
        "# encoder_input_data\n",
        "tokenized_inputs = tokenizer.texts_to_sequences(short_inputs)\n",
        "max_len_in = max([len(x) for x in tokenized_inputs])\n",
        "padded_inputs = preprocessing.sequence.pad_sequences(tokenized_inputs, maxlen=max_len_in, padding='post')\n",
        "encoder_input_data = np.array(padded_inputs)\n",
        "print(encoder_input_data.shape, max_len_in)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "cb8d15a9",
      "metadata": {
        "id": "cb8d15a9",
        "outputId": "eddddace-bf3d-400a-c89d-cb9329872011",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(12682, 9) 9\n"
          ]
        }
      ],
      "source": [
        "# decoder_input_data\n",
        "tokenized_outputs = tokenizer.texts_to_sequences(short_outputs)\n",
        "max_len_out = max([len(x) for x in tokenized_outputs])\n",
        "padded_outputs = preprocessing.sequence.pad_sequences(tokenized_outputs, maxlen=max_len_out, padding='post')\n",
        "decoder_input_data = np.array(padded_outputs)\n",
        "print(decoder_input_data.shape, max_len_out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "b168bff8",
      "metadata": {
        "id": "b168bff8",
        "outputId": "eb8b7355-714c-49b2-8b1b-7c404ea45584",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(12682, 9, 7786)\n"
          ]
        }
      ],
      "source": [
        "# decoder_output_data\n",
        "tokenized_outputs = tokenizer.texts_to_sequences(short_outputs)\n",
        "for i in range(len(tokenized_outputs)) :\n",
        "    tokenized_outputs[i] = tokenized_outputs[i][1:-1]\n",
        "padded_outputs = preprocessing.sequence.pad_sequences(tokenized_outputs, maxlen=max_len_out, padding='post')\n",
        "onehot_outputs = utils.to_categorical(padded_outputs ,VOCAB_SIZE)\n",
        "decoder_output_data = np.array(onehot_outputs)\n",
        "print(decoder_output_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_inputs = tf.keras.layers.Input(shape=(max_len_in , ))\n",
        "encoder_embedding = tf.keras.layers.Embedding( VOCAB_SIZE, 200 , mask_zero=True ) (encoder_inputs)\n",
        "encoder_outputs , state_h , state_c = tf.keras.layers.LSTM( 200 , return_state=True )( encoder_embedding )\n",
        "encoder_states = [ state_h , state_c ]\n",
        "\n",
        "decoder_inputs = tf.keras.layers.Input(shape=( max_len_out ,  ))\n",
        "decoder_embedding = tf.keras.layers.Embedding( VOCAB_SIZE, 200 , mask_zero=True) (decoder_inputs)\n",
        "decoder_lstm = tf.keras.layers.LSTM( 200 , return_state=True , return_sequences=True )\n",
        "decoder_outputs , _ , _ = decoder_lstm ( decoder_embedding , initial_state=encoder_states )\n",
        "decoder_dense = tf.keras.layers.Dense( VOCAB_SIZE , activation=tf.keras.activations.softmax ) \n",
        "output = decoder_dense ( decoder_outputs )\n",
        "\n",
        "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], output )\n",
        "model.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "nyg0G-cjDDgc",
        "outputId": "aedf8eb1-3438-4730-cd00-e34075629dda",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "nyg0G-cjDDgc",
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_15\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_19 (InputLayer)          [(None, 6)]          0           []                               \n",
            "                                                                                                  \n",
            " input_20 (InputLayer)          [(None, 9)]          0           []                               \n",
            "                                                                                                  \n",
            " embedding_2 (Embedding)        (None, 6, 200)       1557200     ['input_19[0][0]']               \n",
            "                                                                                                  \n",
            " embedding_3 (Embedding)        (None, 9, 200)       1557200     ['input_20[0][0]']               \n",
            "                                                                                                  \n",
            " lstm_2 (LSTM)                  [(None, 200),        320800      ['embedding_2[0][0]']            \n",
            "                                 (None, 200),                                                     \n",
            "                                 (None, 200)]                                                     \n",
            "                                                                                                  \n",
            " lstm_3 (LSTM)                  [(None, 9, 200),     320800      ['embedding_3[0][0]',            \n",
            "                                 (None, 200),                     'lstm_2[0][1]',                 \n",
            "                                 (None, 200)]                     'lstm_2[0][2]']                 \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 9, 7786)      1564986     ['lstm_3[0][0]']                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 5,320,986\n",
            "Trainable params: 5,320,986\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "15605348",
      "metadata": {
        "id": "15605348",
        "outputId": "f53b0750-2611-488d-ec39-b7df4ffaf6fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "50/50 [==============================] - 54s 874ms/step - loss: 2.8490 - accuracy: 0.3874\n",
            "Epoch 2/10\n",
            "50/50 [==============================] - 44s 869ms/step - loss: 2.2865 - accuracy: 0.4040\n",
            "Epoch 3/10\n",
            "50/50 [==============================] - 43s 867ms/step - loss: 2.1945 - accuracy: 0.4200\n",
            "Epoch 4/10\n",
            "50/50 [==============================] - 43s 863ms/step - loss: 2.1389 - accuracy: 0.4270\n",
            "Epoch 5/10\n",
            "50/50 [==============================] - 43s 862ms/step - loss: 2.0946 - accuracy: 0.4321\n",
            "Epoch 6/10\n",
            "50/50 [==============================] - 44s 881ms/step - loss: 2.0533 - accuracy: 0.4392\n",
            "Epoch 7/10\n",
            "50/50 [==============================] - 44s 871ms/step - loss: 2.0109 - accuracy: 0.4458\n",
            "Epoch 8/10\n",
            "50/50 [==============================] - 43s 868ms/step - loss: 1.9692 - accuracy: 0.4501\n",
            "Epoch 9/10\n",
            "50/50 [==============================] - 43s 862ms/step - loss: 1.9310 - accuracy: 0.4546\n",
            "Epoch 10/10\n",
            "50/50 [==============================] - 43s 864ms/step - loss: 1.8924 - accuracy: 0.4592\n"
          ]
        }
      ],
      "source": [
        "model.fit([encoder_input_data , decoder_input_data], decoder_output_data, batch_size=256, epochs=10) \n",
        "model.save( 'model.h5' ) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "5e0c85ce",
      "metadata": {
        "id": "5e0c85ce",
        "outputId": "26e5a188-5621-42d2-b06b-f6547d8781c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "accuracy = model.history.history['accuracy']\n",
        "loss = model.history.history['loss']\n",
        "epochs = range(len(accuracy))\n",
        "plt.plot(epochs, accuracy, 'g')\n",
        "plt.plot(epochs, loss, 'b')\n",
        "plt.legend(['Accuracy','Loss'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "52df2d2c",
      "metadata": {
        "id": "52df2d2c"
      },
      "outputs": [],
      "source": [
        "def make_inference_models():\n",
        "    \n",
        "    encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n",
        "    \n",
        "    decoder_state_input_h = tf.keras.layers.Input(shape=( 200 ,))\n",
        "    decoder_state_input_c = tf.keras.layers.Input(shape=( 200 ,))\n",
        "    \n",
        "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "    \n",
        "    decoder_outputs, state_h, state_c = decoder_lstm(\n",
        "        decoder_embedding , initial_state=decoder_states_inputs)\n",
        "    decoder_states = [state_h, state_c]\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "    decoder_model = tf.keras.models.Model(\n",
        "        [decoder_inputs] + decoder_states_inputs,\n",
        "        [decoder_outputs] + decoder_states)\n",
        "    \n",
        "    return encoder_model , decoder_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "eab02a92",
      "metadata": {
        "id": "eab02a92"
      },
      "outputs": [],
      "source": [
        "def str_to_tokens( sentence : str ):\n",
        "    words = sentence.lower().split()\n",
        "    tokens_list = list()\n",
        "    for word in words:\n",
        "        tokens_list.append( tokenizer.word_index[ word ] ) \n",
        "    return preprocessing.sequence.pad_sequences( [tokens_list] , maxlen=max_len_in , padding='post')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "f8af9797",
      "metadata": {
        "id": "f8af9797",
        "outputId": "fbe1ee5d-d4db-48a3-ea4c-524bce730884",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 817
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter question : hello\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 9) for input KerasTensor(type_spec=TensorSpec(shape=(None, 9), dtype=tf.float32, name='input_20'), name='input_20', description=\"created by layer 'input_20'\"), but it was called on an input with incompatible shape (None, 1).\n",
            " the course the minute the the the the the the\n",
            "Enter question : who are you\n",
            " i am not the the the the the the the\n",
            "Enter question : what is your name\n",
            " the little the the the the the the the the\n",
            "Enter question : okay\n",
            " i am the the the the the the the the\n",
            "Enter question : what is the the the\n",
            " the course the minute the the the the the the\n",
            "Enter question : how old are you\n",
            " i am not the the the the the the the\n",
            "Enter question : why are you like this\n",
            " i am not the the the the the the the\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    624\u001b[0m         \"\"\"\n\u001b[0;32m--> 625\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-11f5d4fe740d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mstates_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menc_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mstr_to_tokens\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m'Enter question : '\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mempty_target_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m(\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mempty_target_seq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'go'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "enc_model , dec_model = make_inference_models()\n",
        "\n",
        "for _ in range(10):\n",
        "    states_values = enc_model.predict( str_to_tokens( input( 'Enter question : ' ) ) )\n",
        "    empty_target_seq = np.zeros( ( 1 , 1 ) )\n",
        "    empty_target_seq[0, 0] = tokenizer.word_index['go']\n",
        "    stop_condition = False\n",
        "    decoded_translation = ''\n",
        "    while not stop_condition :\n",
        "        dec_outputs , h , c = dec_model.predict([ empty_target_seq ] + states_values )\n",
        "        sampled_word_index = np.argmax( dec_outputs[0, -1, :] )\n",
        "        sampled_word = None\n",
        "        for word , index in tokenizer.word_index.items() :\n",
        "            if sampled_word_index == index :\n",
        "                decoded_translation += ' {}'.format( word )\n",
        "                sampled_word = word\n",
        "        \n",
        "        if sampled_word == 'eos' or len(decoded_translation.split()) > max_len_out:\n",
        "            stop_condition = True\n",
        "            \n",
        "        empty_target_seq = np.zeros( ( 1 , 1 ) )  \n",
        "        empty_target_seq[ 0 , 0 ] = sampled_word_index\n",
        "        states_values = [ h , c ] \n",
        "\n",
        "    print( decoded_translation )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "88acdfae",
      "metadata": {
        "id": "88acdfae"
      },
      "outputs": [],
      "source": [
        "'''because of the memory error, i trimmed the model here but the accuraccy is not good and chatbot replies only few words mostly the the the'''\n",
        "'''hahah'''"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "preprocessing.ipynb",
      "provenance": []
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}